#!/usr/bin/env python3
"""
é¡¹ç›®çº§æ¼æ´æ£€æµ‹è„šæœ¬
ä½¿ç”¨å®Œæ•´é¡¹ç›®ä½œä¸ºè¾“å…¥è¿›è¡Œå®‰å…¨åˆ†æ
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from evaluation.checkpoint_manager import CheckpointManager
from evaluation.checkpoint_utils import _get_project_data, _format_project_prompt, recompute_metrics, _get_dataset_df, show_summary
from models.llm_client import LLMClient
import argparse
import csv

def detect_project_vulnerabilities(dataset_type: str = "original", sample_size: int = 2, model_name: str = "deepseek-ai/DeepSeek-V3.1", force_restart: bool = False, force_continue: bool = False, only_errors: bool = False, indices: str = ""):
    """
    å¯¹é¡¹ç›®è¿›è¡Œæ¼æ´æ£€æµ‹
    
    Args:
        dataset_type: æ•°æ®é›†ç±»å‹
        sample_size: æ ·æœ¬æ•°é‡
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
        force_restart: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼ˆè·³è¿‡ç¼“å­˜è¯¢é—®ï¼‰
        force_continue: æ˜¯å¦å¼ºåˆ¶ç»§ç»­ç°æœ‰è¯„ä¼°ï¼ˆè·³è¿‡è¯¢é—®ï¼‰
    """
    # åˆå§‹åŒ–ç»„ä»¶ - æŒ‰æ•°æ®é›†åˆ†ç¦»æ–‡ä»¶
    checkpoint_manager = CheckpointManager(model_name=model_name, dataset_type=dataset_type)
    
    # ä¼˜å…ˆä» model_endpoints.json è¯»å–é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨ config.py
    import json
    import os
    model_cfg = None
    
    # 1. å°è¯•ä» model_endpoints.json è¯»å–
    model_endpoints_file = os.path.join(current_dir, "config", "model_endpoints.json")
    if os.path.exists(model_endpoints_file):
        try:
            with open(model_endpoints_file, 'r', encoding='utf-8') as f:
                endpoints_config = json.load(f)
                if model_name in endpoints_config:
                    cfg = endpoints_config[model_name]
                    if not cfg.get('hidden', False):  # è·³è¿‡éšè—çš„æ¨¡å‹
                        model_cfg = {
                            "name": cfg.get("name"),
                            "api_base": cfg.get("api_base"),
                            "api_key": cfg.get("api_key"),
                            "max_tokens": 4096,
                            "timeout": cfg.get("timeout", 60)
                        }
                        print(f"ğŸ“ ä½¿ç”¨ model_endpoints.json ä¸­çš„é…ç½®ï¼ˆè¶…æ—¶: {cfg.get('timeout', 60)}ç§’ï¼‰")
        except Exception as e:
            print(f"âš ï¸ è¯»å– model_endpoints.json å¤±è´¥: {e}")
    
    # 2. å›é€€åˆ° config.py
    if model_cfg is None:
        from config.config import LLM_CONFIG
        for m in LLM_CONFIG.get("models", []):
            if str(m.get("name","")) == str(model_name):
                model_cfg = m
                print(f"ğŸ“ ä½¿ç”¨ config.py ä¸­çš„é…ç½®")
                break
    
    client = LLMClient(model_config=model_cfg)
    # ä»¥GTçœŸå®è¡Œæ•°é™åˆ¶sample_sizeï¼Œé¿å…è¶Šç•Œ
    try:
        ds_df = _get_dataset_df(dataset_type)
        dataset_rows = len(ds_df)
        if sample_size > dataset_rows:
            print(f"âš™ï¸  é‡‡æ ·ä¸Šé™ç”± {sample_size} è°ƒæ•´ä¸º GT è¡Œæ•° {dataset_rows}")
            sample_size = dataset_rows
    except Exception as e:
        print(f"âš ï¸ æ•°æ®é›†è¯»å–å¤±è´¥ï¼Œæ— æ³•é™å¹…sample_size: {e}")
    
    # åŒæ­¥ç»“æœCSVä¸­çš„success/erroråˆ°è¿›åº¦ï¼Œä»¥ä¾¿ï¼š
    # - error ä¸è®¡å…¥å®Œæˆ
    # - ä»…é”™è¯¯é‡è·‘å¯è¦†ç›–åˆ°CSVä¸­çš„é”™è¯¯æ ·æœ¬
    try:
        def _sync_progress_with_results(cm: CheckpointManager):
            results_csv = cm.results_csv
            if not os.path.exists(results_csv):
                return
            try:
                with open(results_csv, 'r', encoding='utf-8', errors='ignore', newline='') as f:
                    rdr = csv.DictReader(f)
                    csv_success = set()
                    csv_error = set()
                    for r in rdr:
                        sid = str(r.get('sample_id', '')).strip()
                        try:
                            sidx = int(str(r.get('sample_index', '0')).strip())
                        except Exception:
                            continue
                        if not sid:
                            continue
                        key = f"{sidx}_{sid}"
                        st = str(r.get('status', '')).strip().lower()
                        if st == 'success':
                            csv_success.add(key)
                        elif st == 'error':
                            csv_error.add(key)
                # å»é‡åˆå¹¶
                completed = list(dict.fromkeys(checkpoint_manager.progress.get('completed_samples', [])))
                failed = list(dict.fromkeys(checkpoint_manager.progress.get('failed_samples', [])))
                # ä» completed ç§»é™¤ä»»ä½•åœ¨ csv_error ä¸­çš„é”®
                completed = [k for k in completed if k not in csv_error]
                # æŠŠ csv_success è¡¥å…¥ completed
                for k in csv_success:
                    if k not in completed:
                        completed.append(k)
                # æŠŠ csv_error è¡¥å…¥ failed
                for k in csv_error:
                    if k not in failed:
                        failed.append(k)
                checkpoint_manager.progress['completed_samples'] = completed
                checkpoint_manager.progress['failed_samples'] = failed
                checkpoint_manager._save_progress()
            except Exception:
                pass
        _sync_progress_with_results(checkpoint_manager)
    except Exception:
        pass

    # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰è¿›åº¦
    progress = checkpoint_manager.get_progress_summary()
    has_existing_progress = progress and progress.get('completed_samples', 0) > 0
    
    if has_existing_progress and not force_restart:
        print("ğŸ” æ£€æµ‹åˆ°ç°æœ‰è¯„ä¼°è¿›åº¦:")
        print(f"  è¯„ä¼°ID: {progress['evaluation_id']}")
        print(f"  æ•°æ®é›†: {progress['dataset_type']}")
        print(f"  æ¨¡å‹: {progress['model_name']}")
        print(f"  å·²å®Œæˆ: {progress.get('completed_samples', 0)} ä¸ªæ ·æœ¬")
        print(f"  å¤±è´¥: {progress.get('failed_samples', 0)} ä¸ªæ ·æœ¬")
        print(f"  æ€»æ ·æœ¬æ•°: {progress['total_samples']}")
        print(f"  è¿›åº¦: {progress['progress_percentage']:.1f}%")
        print(f"  æœ€åæ›´æ–°: {progress['last_updated']}")
        print()
        
        if force_continue:
            print("ğŸ“‹ å¼ºåˆ¶ç»§ç»­ç°æœ‰è¯„ä¼°...")
            evaluation_id = progress['evaluation_id']
            # è‹¥éœ€è¦ï¼Œæ›´æ–°æ€»æ ·æœ¬æ•°
            try:
                if int(checkpoint_manager.progress.get('total_samples', 0)) != int(sample_size):
                    checkpoint_manager.progress['total_samples'] = int(sample_size)
                    checkpoint_manager._save_progress()
            except Exception:
                pass
        else:
            while True:
                choice = input("è¯·é€‰æ‹©æ“ä½œ [C]ç»§ç»­ç°æœ‰è¯„ä¼° / [R]é‡æ–°å¼€å§‹ / [Q]é€€å‡º: ").strip().upper()
                if choice in ['C', 'CONTINUE', 'ç»§ç»­']:
                    print("ğŸ“‹ ç»§ç»­ç°æœ‰è¯„ä¼°...")
                    # ä½¿ç”¨ç°æœ‰çš„evaluation_id
                    evaluation_id = progress['evaluation_id']
                    # åŒæ­¥æ€»æ ·æœ¬æ•°
                    try:
                        if int(checkpoint_manager.progress.get('total_samples', 0)) != int(sample_size):
                            checkpoint_manager.progress['total_samples'] = int(sample_size)
                            checkpoint_manager._save_progress()
                    except Exception:
                        pass
                    break
                elif choice in ['R', 'RESTART', 'é‡æ–°å¼€å§‹']:
                    print("ğŸ”„ æ¸…é™¤ç¼“å­˜å¹¶é‡æ–°å¼€å§‹...")
                    # æ‰‹åŠ¨é‡ç½®è¿›åº¦
                    import os
                    from datetime import datetime
                    progress_file = checkpoint_manager.progress_file
                    if os.path.exists(progress_file):
                        backup_file = f"{progress_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        os.rename(progress_file, backup_file)
                        print(f"ğŸ“‹ è¿›åº¦å·²å¤‡ä»½åˆ°: {backup_file}")
                    
                    # åˆ é™¤ç»“æœCSV
                    results_csv = checkpoint_manager.results_csv
                    if os.path.exists(results_csv):
                        os.remove(results_csv)
                        print("ğŸ—‘ï¸ å·²æ¸…é™¤æ£€æµ‹ç»“æœ")
                    
                    evaluation_id = checkpoint_manager.start_evaluation(dataset_type, model_name, sample_size)
                    break
                elif choice in ['Q', 'QUIT', 'é€€å‡º']:
                    print("ğŸ‘‹ é€€å‡ºæ£€æµ‹")
                    return
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ C/R/Q")
    else:
        # æ²¡æœ‰ç°æœ‰è¿›åº¦æˆ–å¼ºåˆ¶é‡æ–°å¼€å§‹
        if has_existing_progress and force_restart:
            print("ğŸ”„ å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼Œæ¸…é™¤ç°æœ‰ç¼“å­˜...")
            # æ‰‹åŠ¨é‡ç½®è¿›åº¦
            import os
            from datetime import datetime
            progress_file = checkpoint_manager.progress_file
            if os.path.exists(progress_file):
                backup_file = f"{progress_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.rename(progress_file, backup_file)
                print(f"ğŸ“‹ è¿›åº¦å·²å¤‡ä»½åˆ°: {backup_file}")
            
            # åˆ é™¤ç»“æœCSV
            results_csv = checkpoint_manager.results_csv
            if os.path.exists(results_csv):
                os.remove(results_csv)
                print("ğŸ—‘ï¸ å·²æ¸…é™¤æ£€æµ‹ç»“æœ")
        evaluation_id = checkpoint_manager.start_evaluation(dataset_type, model_name, sample_size)
    print(f"ğŸš€ å¼€å§‹é¡¹ç›®çº§æ¼æ´æ£€æµ‹è¯„ä¼°: {evaluation_id}")
    print(f"ğŸ“Š æ•°æ®é›†ç±»å‹: {dataset_type}")
    print(f"ğŸ¤– æ¨¡å‹: {model_name}")
    print(f"ğŸ“ æ ·æœ¬æ•°é‡: {sample_size}")
    print()
    
    # è®¡ç®—å¾…å¤„ç†çš„å·¥ä½œé¡¹ï¼ˆæŒ‰é¡¹ç›®ä¸ç‰ˆæœ¬ç²’åº¦ï¼‰
    work_items = []  # List[Tuple[int, str]] of (sample_index, version)
    # è§£æ indices è¿‡æ»¤
    indices_filter = set()
    if indices:
        try:
            for tok in str(indices).split(','):
                tok = tok.strip()
                if tok:
                    indices_filter.add(int(tok))
        except Exception:
            indices_filter = set()

    if has_existing_progress and evaluation_id == progress['evaluation_id']:
        # ç»§ç»­ç°æœ‰è¯„ä¼°ï¼Œéœ€è¦ä»checkpoint_managerè·å–è¯¦ç»†çš„å·²å®Œæˆæ ·æœ¬é”®
        detailed_progress = checkpoint_manager.progress
        completed_keys = set(detailed_progress.get('completed_samples', []))
        failed_keys = set(detailed_progress.get('failed_samples', []))

        # æ„å»ºå¤±è´¥ç‰ˆæœ¬æ˜ å°„ï¼šindex -> {vulnerable/fixed}
        failed_by_index = {}
        for k in failed_keys:
            try:
                ver = 'vulnerable' if str(k).endswith('_vulnerable') else ('fixed' if str(k).endswith('_fixed') else '')
                idx_str = str(k).split('_', 1)[0]
                idx = int(idx_str)
                if ver:
                    failed_by_index.setdefault(idx, set()).add(ver)
            except Exception:
                continue

        for i in range(sample_size):
            if indices_filter and i not in indices_filter:
                continue
            for version in ["vulnerable", "fixed"]:
                # ä»…é”™è¯¯æ ·æœ¬æ¨¡å¼ï¼šåªåŠ å…¥å¤±è´¥çš„ index/version
                if only_errors:
                    vers = failed_by_index.get(i, set())
                    if version not in vers:
                        continue
                # å¿½ç•¥ project_nameï¼Œä»…æŒ‰ index å’Œ version åˆ¤æ–­å¤„ç†çŠ¶æ€
                prefix = f"{i}_"
                suffix = f"_{version}"
                done_in_completed = any(k.startswith(prefix) and k.endswith(suffix) for k in completed_keys)
                present_in_failed = any(k.startswith(prefix) and k.endswith(suffix) for k in failed_keys)
                # è‹¥åŒæ—¶å­˜åœ¨äºcompletedä¸failedï¼Œè§†ä¸ºéœ€è¦é‡è¯•ï¼ˆä»¥failedä¸ºå‡†ï¼‰
                processed = done_in_completed and not present_in_failed
                if not processed:
                    work_items.append((i, version))

        if not work_items:
            print("âœ… æ‰€æœ‰æ ·æœ¬å·²å¤„ç†å®Œæˆï¼")
            try:
                # æ£€æŸ¥æ˜¯å¦çœŸæ­£å®Œæˆï¼ˆcompleted+failed è¦†ç›– expanded_totalï¼‰
                ps = checkpoint_manager.get_progress_summary()
                remaining = int(ps.get('remaining_samples', 0) or 0)
                if remaining == 0:
                    print("ğŸ“¦ æ­£åœ¨è‡ªåŠ¨è§¦å‘è¯„ä¼°ï¼ˆå…¨é‡ï¼‰...")
                    recompute_metrics(dataset_type=dataset_type or ps.get('dataset_type'))
                    show_summary()
                else:
                    print("â„¹ï¸ ä»æœ‰å‰©ä½™æ ·æœ¬ï¼Œæš‚ä¸è§¦å‘å…¨é‡è¯„ä¼°ã€‚")
            except Exception as e:
                print(f"âš ï¸ è‡ªåŠ¨è¯„ä¼°å¤±è´¥: {e}")
                print(f"   å¯æ‰‹åŠ¨æ‰§è¡Œ: python evaluation/checkpoint_utils.py recompute --dataset-type {dataset_type}")
                print(f"                 python evaluation/checkpoint_utils.py summary")
            return

        pending_by_index = {}
        for idx, ver in work_items:
            pending_by_index.setdefault(idx, []).append(ver)
        print("ğŸ“‹ ç»§ç»­å¤„ç†å‰©ä½™å·¥ä½œé¡¹:")
        for idx in sorted(pending_by_index.keys()):
            print(f"   æ ·æœ¬ {idx}: {pending_by_index[idx]}")
    else:
        # æ–°çš„è¯„ä¼°ï¼Œå…¨éƒ¨åŠ å…¥ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸¤ä¸ªç‰ˆæœ¬ï¼‰
        for i in range(sample_size):
            if indices_filter and i not in indices_filter:
                continue
            for version in ["vulnerable", "fixed"]:
                # æ–°è¯„ä¼°æ—¶ --only-errors æ— æ„ä¹‰ï¼Œå¿½ç•¥
                work_items.append((i, version))
    
    # å¤„ç†å·¥ä½œé¡¹ï¼ˆæŒ‰æ ·æœ¬ä¸ç‰ˆæœ¬ï¼‰
    for widx, (i, version) in enumerate(work_items):
        print(f"ğŸ” å¤„ç†æ ·æœ¬ {i} [{version}] ({widx+1}/{len(work_items)})...")
        try:
                # è·å–é¡¹ç›®æ•°æ®ï¼ˆæŒ‰ç‰ˆæœ¬ï¼‰
                project_data = _get_project_data(dataset_type, i, version=version)
                project_name = project_data['project_name']
                print(f"   é¡¹ç›®: {project_name} [{version}]")
                
                # æ ¼å¼åŒ–æç¤º
                prompt = _format_project_prompt(project_data)
                print(f"   ç”Ÿæˆé¡¹ç›®åˆ†ææç¤º ({len(prompt)} å­—ç¬¦)")
                
                # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                print("   ğŸ¤– è°ƒç”¨LLMåˆ†æé¡¹ç›®...")
                import time
                start_time = time.time()
                
                response = client.detect_vulnerability(prompt)
                processing_time = time.time() - start_time
                
                print(f"   âœ… LLMåˆ†æå®Œæˆ ({processing_time:.2f}s)")
                print(f"   ğŸ“‹ æ£€æµ‹ç»“æœ: {response.get('has_vulnerability', False)}")
                if response.get('vulnerability_type'):
                    print(f"   ğŸ› æ¼æ´ç±»å‹: {response.get('vulnerability_type')}")
                
                # æ„é€ æ ·æœ¬æ•°æ®ï¼ˆåŒºåˆ†ç‰ˆæœ¬ï¼‰
                gt_data = project_data['gt_data']
                is_fixed = (version == "fixed")
                sample_data = {
                    'index': i,
                    'id': f"{project_name}_{version}",
                    'has_vulnerability': False if is_fixed else (gt_data.get('cwe_ids') not in [None, '', 'None']),
                    'vulnerability_classification': '' if is_fixed else gt_data.get('cwe_ids', ''),
                    'vulnerable_code_paths': gt_data.get('vulnerable_code_paths', ''),
                    'vulnerable_function_names': gt_data.get('vulnerable_function_names', ''),
                    'file_path': '',
                    'function_name': gt_data.get('vulnerable_function_names', '').split(',')[0].strip() if gt_data.get('vulnerable_function_names') else '__file_scope__',
                    # æ·»åŠ é¡¹ç›®çº§å…ƒæ•°æ®
                    'project_type': gt_data.get('project_type', ''),
                    'cve_ids': gt_data.get('cve_ids', ''),
                    'code_links': gt_data.get('code_links', ''),
                    'sources': gt_data.get('sources', ''),
                    'severity_breakdown': gt_data.get('severity_breakdown', ''),
                    'vulnerability_classification_breakdown': gt_data.get('vulnerability_classification_breakdown', ''),
                    'files': gt_data.get('files', ''),
                    'function_label_breakdown': gt_data.get('function_label_breakdown', ''),
                    'commit_shas': gt_data.get('commit_shas', ''),
                    'publish_date_last': gt_data.get('publish_date_last', ''),
                    'fixed_code_paths': gt_data.get('fixed_code_paths', ''),
                    'summaries_merged': gt_data.get('summaries_merged', ''),
                    'vulnerable_line_ranges': gt_data.get('vulnerable_line_ranges', ''),
                    'project_type_breakdown': gt_data.get('project_type_breakdown', ''),
                }
                
                # æå–é¢„æµ‹æŒ‡æ ‡
                metrics = {
                    'has_vulnerability_pred': response.get('has_vulnerability', False),
                    'vulnerability_type_pred': response.get('vulnerability_type', ''),
                    'filename_pred': response.get('filename', ''),
                    'function_name_pred': response.get('function_name', ''),
                    'vulnerable_lines_pred': response.get('vulnerable_lines', []),
                }

                # åˆ¤å®šæ˜¯å¦ä¸ºâ€œNoneæ ·â€å¤±è´¥ï¼ˆå¦‚APIå¤±è´¥çš„å ä½è¿”å›ï¼‰ï¼šç±»å‹ä¸ºNone/ç©ºä¸”æ— æ¼æ´ï¼Œä¸”åŸå§‹å“åº”ä¸ºç©ºæˆ–è§£é‡ŠåŒ…å«APIå¤±è´¥æç¤º
                def _is_none_like_failure(resp: dict) -> bool:
                    try:
                        vt = str(resp.get('vulnerability_type', '')).strip().lower()
                        hv = bool(resp.get('has_vulnerability', False))
                        expl = str(resp.get('explanation', '')).replace('\n', ' ').strip()
                        raw = str(resp.get('raw_response', '')).strip()
                        return ((vt in ('none', '')) and (not hv) and (not raw or 'apiè°ƒç”¨å¤±è´¥' in expl))
                    except Exception:
                        return False

                status_flag = "error" if _is_none_like_failure(response) else "success"
                error_msg = "LLM API failure or empty fields (None-like response)" if status_flag == "error" else ""

                # ä¿å­˜ç»“æœï¼ˆæŒ‰åˆ¤å®šå†™å…¥æˆåŠŸ/å¤±è´¥ï¼‰
                success = checkpoint_manager.save_sample_result(
                    sample_data, metrics, str(response), processing_time, status_flag, error_msg
                )
                
                if success:
                    if status_flag == "error":
                        print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜ï¼ˆæ ‡è®°ä¸ºå¤±è´¥ï¼‰")
                    else:
                        print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜")
                else:
                    print(f"   âŒ ä¿å­˜å¤±è´¥")
                    
        except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                # ä¿å­˜é”™è¯¯è®°å½•
                error_sample_data = {
                    'index': i,
                    'id': f'{project_name}_{version}' if 'project_name' in locals() else f'sample_{i}_{version}',
                    'has_vulnerability': False,
                    'vulnerability_classification': '',
                    'vulnerable_code_paths': '',
                    'vulnerable_function_names': '',
                    'file_path': '',
                    'function_name': '',
                }
                error_metrics = {
                    'has_vulnerability_pred': False,
                    'vulnerability_type_pred': '',
                    'filename_pred': '',
                    'function_name_pred': '',
                    'vulnerable_lines_pred': [],
                }
                checkpoint_manager.save_sample_result(
                    error_sample_data, error_metrics, str(e), 0.0, "error", str(e)
                )
            
        print()
    
    print(f"ğŸ‰ æ£€æµ‹å®Œæˆï¼")
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å®Œæˆæ¡ä»¶ï¼ˆcompleted+failed è¦†ç›– expanded_totalï¼‰ï¼Œè‹¥æ˜¯åˆ™æ‰§è¡Œå…¨é‡è¯„ä¼°ï¼›å¦åˆ™ä¸è§¦å‘
    try:
        ps = checkpoint_manager.get_progress_summary()
        remaining = int(ps.get('remaining_samples', 0) or 0)
        if remaining == 0:
            print("ğŸ“¦ æ­£åœ¨è‡ªåŠ¨è§¦å‘è¯„ä¼°ï¼ˆå…¨é‡ï¼‰...")
            recompute_metrics(dataset_type=dataset_type or ps.get('dataset_type'))
            show_summary()
            print("âœ… è¯„ä¼°å®Œæˆã€‚")
        else:
            print("â„¹ï¸ å°šæœªè¦†ç›–å…¨éƒ¨æ ·æœ¬ï¼Œè·³è¿‡å…¨é‡è¯„ä¼°è§¦å‘ã€‚")
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨è¯„ä¼°å¤±è´¥: {e}")
        print(f"   å¯æ‰‹åŠ¨æ‰§è¡Œ: python evaluation/checkpoint_utils.py recompute --dataset-type {dataset_type}")
        print(f"                 python evaluation/checkpoint_utils.py summary")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="é¡¹ç›®çº§æ¼æ´æ£€æµ‹")
    parser.add_argument("--dataset-type", type=str, default="original", 
                       choices=["original", "noise", "obfuscated", "noise_obfuscated", "prompt_injection"],
                       help="æ•°æ®é›†ç±»å‹")
    parser.add_argument("--sample-size", type=int, default=2, help="æ ·æœ¬æ•°é‡")
    parser.add_argument("--model", type=str, default="deepseek-ai/DeepSeek-V3.1", help="æ¨¡å‹åç§°")
    parser.add_argument("--force-restart", action="store_true", 
                       help="å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼Œè·³è¿‡ç¼“å­˜è¯¢é—®ç›´æ¥æ¸…é™¤æ‰€æœ‰è¿›åº¦")
    parser.add_argument("--continue", dest="force_continue", action="store_true",
                       help="å¼ºåˆ¶ç»§ç»­ç°æœ‰è¯„ä¼°ï¼Œè·³è¿‡è¯¢é—®")
    parser.add_argument("--only-errors", action="store_true", help="ä»…é‡è·‘å¤±è´¥æ ·æœ¬ï¼ˆéœ€å­˜åœ¨å†å²è¿›åº¦ï¼‰")
    parser.add_argument("--indices", type=str, default="", help="ä»…å¤„ç†æŒ‡å®šç´¢å¼•ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œä¾‹å¦‚: 0,5,12")
    args = parser.parse_args()

    # å¤„ç†äº’æ–¥å‚æ•°
    if args.force_restart and args.force_continue:
        print("âŒ --force-restart å’Œ --continue å‚æ•°ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        sys.exit(1)

    detect_project_vulnerabilities(
        dataset_type=args.dataset_type,
        sample_size=args.sample_size,
        model_name=args.model,
        force_restart=args.force_restart,
        force_continue=args.force_continue,
        only_errors=args.only_errors,
        indices=args.indices,
    )
