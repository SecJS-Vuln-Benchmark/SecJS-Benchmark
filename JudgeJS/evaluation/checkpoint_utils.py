#!/usr/bin/env python3
"""
æ–­ç‚¹ç»­ä¼ ç®¡ç†å·¥å…·
æä¾›å‘½ä»¤è¡Œæ¥å£æ¥ç®¡ç†è¯„ä¼°æ£€æŸ¥ç‚¹
"""

import argparse
import os
import sys
import json
from datetime import datetime
import csv
import pandas as pd
import shutil
import re
from typing import Dict, Any, List

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from evaluation.checkpoint_manager import CheckpointManager
from evaluation.metrics import summarize_confusions, similarity_rates, similarity_means
from evaluation.metrics import compute_project_tuple_confusion, compute_function_quad_confusion, precision_recall_f1
from config.config import DATA_CONFIG, CONSENSUS_CONFIG
from models.llm_client import LLMClient
from evaluation.metrics import build_metrics_for_sample
from utils.project_loader import ProjectLoader
import os as _os_for_prompt
import sys as _sys_for_prompt
# æ·»åŠ  claude-code-security-review åˆ°è·¯å¾„
_claude_path = _os_for_prompt.path.join(_os_for_prompt.path.dirname(_os_for_prompt.path.dirname(_os_for_prompt.path.abspath(__file__))), 'claude-code-security-review')
if _os_for_prompt.path.exists(_claude_path):
    _sys_for_prompt.path.append(_claude_path)
    from claudecode.prompts import get_vulnerability_detection_prompt
else:
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä»çˆ¶ç›®å½•æŸ¥æ‰¾
    _parent_claude_path = _os_for_prompt.path.join(_os_for_prompt.path.dirname(_os_for_prompt.path.dirname(_os_for_prompt.path.dirname(_os_for_prompt.path.abspath(__file__)))), 'claude-code-security-review')
    if _os_for_prompt.path.exists(_parent_claude_path):
        _sys_for_prompt.path.append(_parent_claude_path)
        from claudecode.prompts import get_vulnerability_detection_prompt
    else:
        raise ImportError("claude-code-security-review æ¨¡å—æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è¯¥æ¨¡å—å­˜åœ¨äº JudgeJS ç›®å½•æˆ–çˆ¶ç›®å½•ä¸­ã€‚")


def show_progress(model_name: str = "deepseek-ai/DeepSeek-V3.1", dataset_type: str = "original", include_errors_in_remaining: bool = False):
    """æ˜¾ç¤ºå½“å‰è¯„ä¼°è¿›åº¦ï¼ˆæŒ‰æ¨¡å‹å’Œæ•°æ®é›†ï¼‰"""
    checkpoint_manager = CheckpointManager(model_name=model_name, dataset_type=dataset_type)
    progress = checkpoint_manager.get_progress_summary(include_errors_in_remaining=include_errors_in_remaining)
    
    if progress and progress.get('evaluation_id'):
        print("ğŸ“Š å½“å‰è¯„ä¼°è¿›åº¦:")
        print(f"  è¯„ä¼°ID: {progress['evaluation_id']}")
        print(f"  æ•°æ®é›†: {progress['dataset_type']}")
        print(f"  æ¨¡å‹: {progress['model_name']}")
        print(f"  æ€»æ ·æœ¬æ•°: {progress['total_samples']}")
        print(f"  å·²å®Œæˆ: {progress['completed_samples']}")
        if progress.get('include_errors_in_remaining'):
            print(f"  å¤±è´¥: 0 (å·²å¹¶å…¥å‰©ä½™)")
            print(f"  å‰©ä½™: {progress['remaining_samples']} (å«error)")
        else:
            print(f"  å¤±è´¥: {progress['failed_samples']}")
            print(f"  å‰©ä½™: {progress['remaining_samples']}")
        print(f"  è¿›åº¦: {progress['progress_percentage']:.1f}%")
        print(f"  å¼€å§‹æ—¶é—´: {progress['start_time']}")
        print(f"  æœ€åæ›´æ–°: {progress['last_updated']}")
    else:
        print("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°è¿›åº¦")


def sync_progress_from_results(model_name: str = "deepseek-ai/DeepSeek-V3.1", dataset_type: str = "original"):
    """å°† results CSV ä¸­çš„ success/error åŒæ­¥åˆ°è¿›åº¦æ–‡ä»¶ï¼š
    - success â†’ åŠ å…¥ completed_samples
    - error â†’ ä» completed_samples å‰”é™¤å¹¶åŠ å…¥ failed_samples
    """
    cm = CheckpointManager(model_name=model_name, dataset_type=dataset_type)
    results_csv = cm.results_csv
    if not os.path.exists(results_csv):
        print("â„¹ï¸ æœªæ‰¾åˆ°ç»“æœCSVï¼Œè·³è¿‡åŒæ­¥")
        return
    try:
        with open(results_csv, 'r', encoding='utf-8', errors='ignore', newline='') as f:
            rdr = csv.DictReader(f)
            csv_success = set()
            csv_error = set()
            for r in rdr:
                sid = str(r.get('sample_id', '')).strip()
                try:
                    sidx = int(str(r.get('sample_index', '0')).strip())
                except Exception:
                    continue
                if not sid:
                    continue
                key = f"{sidx}_{sid}"
                st = str(r.get('status', '')).strip().lower()
                if st == 'success':
                    csv_success.add(key)
                elif st == 'error':
                    csv_error.add(key)
        completed = list(dict.fromkeys(cm.progress.get('completed_samples', [])))
        failed = list(dict.fromkeys(cm.progress.get('failed_samples', [])))
        # ä» completed ç§»é™¤ä»»ä½•åœ¨ csv_error ä¸­çš„é”®
        completed = [k for k in completed if k not in csv_error]
        # success è¡¥å…¥ completed
        for k in csv_success:
            if k not in completed:
                completed.append(k)
        # error è¡¥å…¥ failed
        for k in csv_error:
            if k not in failed:
                failed.append(k)
        cm.progress['completed_samples'] = completed
        cm.progress['failed_samples'] = failed
        cm._save_progress()
        print(f"âœ… å·²åŒæ­¥è¿›åº¦ï¼šsuccess={len(csv_success)} error={len(csv_error)}")
    except Exception as e:
        print(f"âŒ åŒæ­¥å¤±è´¥: {e}")


def show_results():
    """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
    checkpoint_manager = CheckpointManager()
    results_csv = checkpoint_manager.results_csv
    
    if os.path.exists(results_csv):
        try:
            df = pd.read_csv(results_csv)
            print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ ({len(df)} æ¡è®°å½•):")
            print(f"  ç»“æœæ–‡ä»¶: {results_csv}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if len(df) > 0:
                print(f"  æˆåŠŸæ ·æœ¬: {len(df[df['status'] == 'success'])}")
                print(f"  å¤±è´¥æ ·æœ¬: {len(df[df['status'] == 'error'])}")
                
                # æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡ï¼ˆç§»é™¤æ—§é€å­—æ®µå‡†ç¡®ç‡å±•ç¤ºï¼‰
                if 'cosine_similarity' in df.columns:
                    print(f"  å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {df['cosine_similarity'].mean():.3f}")
                
                # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•
                print("\n  å‰5æ¡è®°å½•:")
                for i, row in df.head().iterrows():
                    print(f"    {i+1}. æ ·æœ¬{row['sample_index']} - çŠ¶æ€: {row['status']}")
                    # ä¸å†æ˜¾ç¤ºæ—§çš„accuracy/f1å­—æ®µ
        except Exception as e:
            print(f"âŒ è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    else:
        print("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")


def show_summary(model_name: str = "deepseek-ai/DeepSeek-V3.1", dataset_type: str = "original"):
    """æ±‡æ€»å½“å‰ç»“æœCSVï¼Œè¾“å‡ºé¡¹ç›®çº§/å‡½æ•°çº§çš„TP/TN/FP/FNä¸P/R/F1ï¼Œä»¥åŠè¯­ä¹‰ç›¸ä¼¼ç‡ã€‚"""
    checkpoint_manager = CheckpointManager(model_name=model_name, dataset_type=dataset_type)
    results_csv = checkpoint_manager.results_csv
    if not os.path.exists(results_csv):
        print("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")
        return
    try:
        try:
            df = pd.read_csv(results_csv)
        except Exception:
            df = pd.read_csv(results_csv, engine='python', on_bad_lines='skip')
        # ä»…ç»Ÿè®¡æˆåŠŸæ ·æœ¬
        if 'status' in df.columns:
            df = df[df['status'].astype(str).str.lower() == 'success']
        rows = df.to_dict(orient='records')
        proj = summarize_confusions(rows, 'project')
        func = summarize_confusions(rows, 'function')
        sims_mean = similarity_means(rows)
        print("\n===== Summary (Current Results) =====")
        print("Project-level:")
        print(f"  TP: {proj['project_tp']}  TN: {proj['project_tn']}  FP: {proj['project_fp']}  FN: {proj['project_fn']}")
        print(f"  Precision: {proj['project_precision']:.3f}  Recall: {proj['project_recall']:.3f}  F1: {proj['project_f1']:.3f}")
        print("Function-level:")
        print(f"  TP: {func['function_tp']}  TN: {func['function_tn']}  FP: {func['function_fp']}  FN: {func['function_fn']}")
        print(f"  Precision: {func['function_precision']:.3f}  Recall: {func['function_recall']:.3f}  F1: {func['function_f1']:.3f}")
        print("Similarity means:")
        print(f"  Cosine mean: {sims_mean['cosine_similarity_mean']:.3f}")
        print(f"  BERT mean:   {sims_mean['bert_similarity_mean']:.3f}")
        print(f"  ROUGE mean:  {sims_mean['rouge_score_mean']:.3f}")
    except Exception as e:
        print(f"âŒ æ±‡æ€»å¤±è´¥: {e}")


def _get_dataset_df(dataset_type: str) -> pd.DataFrame:
    dt = dataset_type or DATA_CONFIG.get('default_dataset_type', 'original')
    ds_cfg = DATA_CONFIG['dataset_types'].get(dt)
    if not ds_cfg:
        raise RuntimeError(f"æœªçŸ¥æ•°æ®é›†ç±»å‹: {dt}")
    path = ds_cfg['path']
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ•°æ®é›†ä¸å­˜åœ¨: {path}")
    # å°è¯•å¤šç§è¯»å–æ–¹å¼ï¼Œæœ€å¤§åŒ–å…¼å®¹
    try:
        return pd.read_csv(path)
    except Exception:
        try:
            return pd.read_csv(path, engine='python', on_bad_lines='skip')
        except Exception:
            # é€€åŒ–ä¸ºåˆ†å—è¯»å–
            chunks = []
            for chunk in pd.read_csv(path, engine='python', on_bad_lines='skip', chunksize=1000):
                chunks.append(chunk)
            if chunks:
                return pd.concat(chunks, ignore_index=True)
            raise


def _get_project_data(dataset_type: str, sample_index: int, version: str = "vulnerable") -> Dict[str, Any]:
    """
    è·å–é¡¹ç›®çº§æ•°æ®ç”¨äºLLMåˆ†æ
    
    Args:
        dataset_type: æ•°æ®é›†ç±»å‹
        sample_index: æ ·æœ¬ç´¢å¼•
        version: é¡¹ç›®ç‰ˆæœ¬ï¼ˆ"vulnerable" æˆ– "fixed"ï¼‰
        
    Returns:
        åŒ…å«é¡¹ç›®ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–æ•°æ®é›†é…ç½®
    dt = dataset_type or DATA_CONFIG.get('default_dataset_type', 'original')
    ds_cfg = DATA_CONFIG['dataset_types'].get(dt)
    if not ds_cfg:
        raise RuntimeError(f"æœªçŸ¥æ•°æ®é›†ç±»å‹: {dt}")
    
    # è·å–GTæ•°æ®
    df = _get_dataset_df(dataset_type)
    if sample_index >= len(df):
        raise IndexError(f"æ ·æœ¬ç´¢å¼• {sample_index} è¶…å‡ºæ•°æ®é›†èŒƒå›´ [0, {len(df)-1}]")
    
    gt_row = df.iloc[sample_index]
    project_name = str(gt_row.get('project_name', '') or '').strip()
    cve_ids = str(gt_row.get('cve_ids', '') or '').strip()

    import re as _re
    # 1) æ ¡éªŒå¹¶æå– owner_repo
    # æ”¯æŒ 'owner/repo' æˆ– ç›´æ¥ 'owner_repo'
    if '/' in project_name:
        owner, _, repo = project_name.partition('/')
    else:
        parts = project_name.split('_')
        if len(parts) >= 2:
            owner, repo = parts[0], parts[1]
        else:
            raise ValueError(f"[GT invalid] sample_index={sample_index}: project_name malformed: '{project_name}'")
    if not owner or not repo:
        raise ValueError(f"[GT invalid] sample_index={sample_index}: project_name missing owner/repo: '{project_name}'")
    allowed = _re.compile(r'^[A-Za-z0-9_.-]{1,64}$')
    if not allowed.match(owner) or not allowed.match(repo):
        raise ValueError(f"[GT invalid] sample_index={sample_index}: project_name contains illegal chars: '{project_name}'")

    owner_repo = f"{owner}_{repo}"

    # 2) é€‰å–åˆæ³•çš„ CVEï¼ˆä»…å–é¦–ä¸ªåŒ¹é…ï¼‰
    m = _re.search(r'(CVE-\d{4}-\d+)', cve_ids)
    if not m:
        raise ValueError(f"[GT invalid] sample_index={sample_index}: cve_ids malformed: '{cve_ids}'")
    cve = m.group(1)

    # 3) æ‹¼æ¥ä¸¥æ ¼ç›®å½•åå¹¶éªŒè¯å­˜åœ¨æ€§
    actual_project_dir = f"{owner_repo}_{cve}"
    
    # åˆå§‹åŒ–é¡¹ç›®åŠ è½½å™¨
    projects_dir = ds_cfg.get('projects_dir', '../ArenaJS/projects')
    loader = ProjectLoader(projects_dir)

    # ç›®å½•å­˜åœ¨æ€§æ£€æŸ¥ï¼ˆä¸å›é€€ï¼‰
    def _dir_exists(dir_name: str) -> bool:
        try:
            p = loader.get_project_path(dir_name, version)
            return p is not None
        except Exception:
            return False
    if not _dir_exists(actual_project_dir):
        raise FileNotFoundError(f"[GT mismatch] sample_index={sample_index}: project directory not found: '{actual_project_dir}' under '{projects_dir}'")

    # åŠ è½½é¡¹ç›®æ•°æ®ï¼ˆæ”¯æŒ vulnerable/fixed ä¸¤ä¸ªç‰ˆæœ¬ï¼‰
    project_data = loader.load_project_for_analysis(actual_project_dir, version)
    if not project_data:
        # å¦‚æœé¡¹ç›®åŠ è½½å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
        return {
            'project_name': project_name,
            'project_description': f'é¡¹ç›® {project_name}',
            'file_structure': 'é¡¹ç›®ç»“æ„æ— æ³•è·å–',
            'source_files': 'æºä»£ç æ— æ³•è·å–',
            'gt_data': gt_row.to_dict()
        }
    
    # æ·»åŠ GTæ•°æ®
    project_data['gt_data'] = gt_row.to_dict()
    project_data['version'] = version
    return project_data


def _format_project_prompt(project_data: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–é¡¹ç›®æ•°æ®ä¸ºLLMæç¤º
    
    Args:
        project_data: é¡¹ç›®æ•°æ®å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–åçš„æç¤ºå­—ç¬¦ä¸²
    """
    return get_vulnerability_detection_prompt(project_data)


def _normalize_text(s: str) -> str:
    return (str(s) if s is not None else '').strip().lower()


def _normalize_project_key(name: str) -> str:
    return _normalize_text(name).replace('/', '_')


def _split_list_field(s: str) -> list:
    if s is None:
        return []
    return [x.strip() for x in str(s).split(';') if str(x).strip()]


def _basename(p: str) -> str:
    try:
        return os.path.basename(str(p).strip())
    except Exception:
        return str(p).strip()


def _align_gt_row(det_row: Dict[str, Any], df: pd.DataFrame) -> Dict[str, Any]:
    """
    Simple GT alignment: only use sample_index if valid, no complex matching.
    """
    if df is None or df.empty:
        return {}
    
    # Only use sample_index direct match if it's a valid integer within dataset range
    try:
        idx = int(det_row.get('sample_index', -1))
        if 0 <= idx < len(df):
            return df.iloc[idx].to_dict()
    except Exception:
        pass
    
    # If sample_index is invalid or out of range, return empty dict
    return {}


def recompute_metrics(dataset_type: str = None, sample: bool = False, sample_from_dataset: bool = False, sample_size: int = 2, model_name: str = "deepseek-ai/DeepSeek-V3.1"):
    """è¯»å–å½“å‰æ¨¡å‹ç»“æœCSVï¼ŒåŸºäºGTä¸llm_responseé‡ç®—æ–°å¢æŒ‡æ ‡å¹¶å›å†™CSVã€‚"""
    if not dataset_type:
        dataset_type = "original"  # é»˜è®¤ä½¿ç”¨original
    checkpoint_manager = CheckpointManager(model_name=model_name, dataset_type=dataset_type)
    results_csv = checkpoint_manager.results_csv
    if not os.path.exists(results_csv):
        print("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")
        return
    df = None
    try:
        df = _get_dataset_df(dataset_type or checkpoint_manager.progress.get('dataset_type', DATA_CONFIG.get('default_dataset_type')))
    except Exception as e:
        print(f"âš ï¸ æ•°æ®é›†è¯»å–å¤±è´¥ï¼Œå°†ä»¥æ£€æµ‹CSVè‡ªèº«å­—æ®µä¸ºå‡†ç»§ç»­é‡ç®—: {e}")
    try:
        res = pd.read_csv(results_csv)
        # ä»…å¯¹æˆåŠŸæ ·æœ¬é‡ç®—ä¸èšåˆ
        if 'status' in res.columns:
            res = res[res['status'].astype(str).str.lower() == 'success']
        if sample:
            take_n = max(1, int(sample_size))
            selected_sample_indices = set()
            if sample_from_dataset and df is not None and not df.empty:
                # é€‰å–æ•°æ®é›†å‰Nä¸ª sample_index
                selected_sample_indices = set(list(df.index)[:take_n])
                wanted = [str(i) for i in selected_sample_indices]
                mask = res['sample_index'].astype(str).isin(wanted) if 'sample_index' in res.columns else pd.Series([False]*len(res))
                sel = res[mask]
                if sel.empty:
                    target_indices = set(res.head(take_n).index.tolist())
                    # å›é€€ï¼šä»ç»“æœä¸­é€‰å–å‰Nä¸ªå”¯ä¸€ sample_index
                    unique_idx = []
                    for _, r in res.iterrows():
                        try:
                            si = int(r.get('sample_index'))
                        except Exception:
                            continue
                        if si not in unique_idx:
                            unique_idx.append(si)
                        if len(unique_idx) >= take_n:
                            break
                    selected_sample_indices = set(unique_idx)
                else:
                    target_indices = set(sel.index.tolist())
            else:
                # æ— æ•°æ®é›†æ—¶ï¼Œä»ç»“æœä¸­é€‰å–å‰Nä¸ªå”¯ä¸€ sample_index
                target_indices = set()
                unique_idx = []
                for i, r in res.iterrows():
                    try:
                        si = int(r.get('sample_index'))
                    except Exception:
                        continue
                    if si not in unique_idx:
                        unique_idx.append(si)
                        selected_sample_indices.add(si)
                    if len(unique_idx) >= take_n:
                        break
                    target_indices.add(i)
        else:
            target_indices = set(res.index.tolist())
            selected_sample_indices = set(res['sample_index'].dropna().astype(int).unique().tolist()) if 'sample_index' in res.columns else set()
        client = LLMClient()
        updated_rows: List[Dict[str, Any]] = []
        for i, row in res.iterrows():
            if i not in target_indices:
                updated_rows.append(row)
                continue
            gt_row = _align_gt_row(row, df) if df is not None else {}
            # æ„é€  sample_data
            sample_data: Dict[str, Any] = {
                'has_vulnerability': bool(row.get('has_vulnerability_gt', False)),
                'vulnerability_classification': str(row.get('vulnerability_type_gt', '') or (gt_row.get('vulnerability_classification_breakdown', '') if gt_row else '')),
                'vulnerable_code_paths': str(gt_row.get('vulnerable_code_paths', '')) if gt_row else '',
                'vulnerable_function_names': str(gt_row.get('vulnerable_function_names', '')) if gt_row else '',
                'file_path': str(gt_row.get('file_path', '')) if gt_row else '',
                'function_name': str(gt_row.get('function_name', '')) if gt_row else '',
                'files': str(gt_row.get('files', '')) if gt_row else '',
                # ä¾›ä¼ ç»ŸF1ä¸­çš„â€œå™ªå£°å‡½æ•°å‘½ä¸­å³åŠ ä¸¥â€é€»è¾‘ä½¿ç”¨
                'dataset_type': str(dataset_type or ''),
                'sample_id': str(row.get('sample_id', '') or ''),
                'function_names': str(gt_row.get('function_names', '') if gt_row else ''),
            }
            # è§£æé¢„æµ‹
            llm_result: Dict[str, Any] = {
                'has_vulnerability': bool(row.get('has_vulnerability_pred', False)),
                'vulnerability_type': str(row.get('vulnerability_type_pred', '')),
                'filename': str(row.get('filename_pred', '')),
                'function_name': str(row.get('function_name_pred', '')),
            }
            # å¦‚æœç¼ºå¤±ï¼Œå°è¯•ä» llm_response è§£æï¼›åŒæ—¶è·å– explanation çº¯æ–‡æœ¬
            parsed = None
            if (not llm_result['vulnerability_type']) or (not llm_result['filename'] and not llm_result['function_name']):
                parsed = client._parse_detection_response(str(row.get('llm_response', '')))
                if parsed:
                    llm_result.update({k: v for k, v in parsed.items() if k in {'has_vulnerability', 'vulnerability_type', 'filename', 'function_name'}})
            else:
                # ä¹Ÿå°è¯•è§£æä»¥æå– explanation ç”¨äºç›¸ä¼¼åº¦
                parsed = client._parse_detection_response(str(row.get('llm_response', '')))

            gt_text = str(gt_row.get('summaries_merged', '')) if gt_row else ''
            pred_text = str(parsed.get('explanation', '')) if parsed else ''
            m = build_metrics_for_sample(sample_data, llm_result, gt_text, pred_text)
            # å¯é€‰ï¼šè®¡ç®—å•è¡Œçš„precision/recall/f1ä¸å¯è¡Œï¼ˆéœ€èšåˆï¼‰ï¼Œæ­¤å¤„ä»…å­˜è´¡çŒ®
            for k, v in m.items():
                row[k] = v
            # å›å¡«è§£æå¾—åˆ°ä½†CSVä¸­ç¼ºå¤±çš„é¢„æµ‹åˆ—å€¼ï¼ˆä»¥ä¾¿å¯¼å‡ºï¼‰
            if parsed:
                if not row.get('vulnerability_type_pred') and parsed.get('vulnerability_type'):
                    row['vulnerability_type_pred'] = parsed.get('vulnerability_type')
                if not row.get('filename_pred') and parsed.get('filename'):
                    row['filename_pred'] = parsed.get('filename')
                if not row.get('function_name_pred') and parsed.get('function_name'):
                    row['function_name_pred'] = parsed.get('function_name')
            # sample æ¨¡å¼æ ‡è®° evaluation_id
            if sample:
                ev = str(row.get('evaluation_id', ''))
                if not ev.endswith('_sample'):
                    row['evaluation_id'] = (ev + '_sample') if ev else 'eval_sample'
            updated_rows.append(row)
        # è¿½åŠ èšåˆP/R/F1ï¼ˆæ¯è¡Œæ”¾ç©ºï¼Œèšåˆç”±summaryå‘½ä»¤è®¡ç®—ï¼‰ï¼›ä¿æŒå†…éƒ¨ results_csv åˆ—ç¨³å®šï¼ˆä¸åœ¨æ­¤å¤„åˆ é™¤åˆ—ï¼‰
        res_updated = pd.DataFrame(updated_rows)
        res_updated.to_csv(results_csv, index=False)
        print("âœ… å·²åŸºäºGTä¸LLMå“åº”é‡ç®—å¹¶æ›´æ–°æŒ‡æ ‡åˆ—" + ("ï¼ˆsampleæ¨¡å¼ï¼‰" if sample else ""))

        # å‘½åå¯¼å‡ºï¼šæŒ‰æ¨¡å‹åè¾“å‡º evaluation / detection å‘½åæ–‡ä»¶
        try:
            if not res_updated.empty and 'model_name' in res_updated.columns:
                model_name = str(res_updated.iloc[0].get('model_name', 'model')).strip() or 'model'
            else:
                model_name = 'model'
            safe_model = model_name.replace('/', '_').replace(':', '_').replace(' ', '_')
            sample_suffix = '_sample' if (sample or any(str(x).endswith('_sample') for x in res_updated.get('evaluation_id', []))) else ''
            export_dir = os.path.join('evaluation', 'exports')
            os.makedirs(export_dir, exist_ok=True)
            # 1) detection å¯¼å‡ºï¼šsample æ¨¡å¼å¯¼å‡ºæ‰€é€‰ sample_index çš„æ‰€æœ‰è¡Œï¼ˆä¸¤é¡¹ç›®Ã—ä¸¤ä¸ªç‰ˆæœ¬=4è¡Œï¼‰ï¼›å¦åˆ™å¯¼å‡ºåŸå§‹æ¨¡å‹ç»“æœ
            det_src = os.path.join('evaluation', 'checkpoints', 'models', safe_model, 'results', 'evaluation_results.csv')
            # dataset_type for naming
            dataset_type = str(res_updated.iloc[0].get('dataset_type','')) if not res_updated.empty else ''
            safe_dt = (dataset_type.strip() or 'dataset').replace('/', '_').replace(':', '_').replace(' ', '_')
            det_out = os.path.join(export_dir, f"{safe_model}_{safe_dt}_detection{sample_suffix}.csv")
            try:
                if os.path.exists(det_src):
                    det_df = pd.read_csv(det_src)
                else:
                    det_df = res_updated.copy()
                
                # æ·»åŠ GTåŸå§‹å­—æ®µ
                try:
                    gt_df = _get_dataset_df(dataset_type)
                    if not gt_df.empty and len(det_df) > 0:
                        # GTåŸå§‹å­—æ®µåˆ—è¡¨
                        gt_columns = [
                            'project_type', 'cve_ids', 'code_links', 'n_code_links', 'sources',
                            'severity_breakdown', 'vulnerability_classification_breakdown', 
                            'files', 'function_label_breakdown', 'commit_shas', 'publish_date_last',
                            'vulnerable_code_paths', 'fixed_code_paths', 'summaries_merged', 
                            'vulnerable_line_ranges', 'vulnerable_function_names', 'project_type_breakdown'
                        ]
                        # ä¸ºæ¯ä¸ªdetectionè¡Œæ·»åŠ GTåŸå§‹å­—æ®µ
                        for i, row in det_df.iterrows():
                            sample_idx = row.get('sample_index')
                            if pd.notna(sample_idx) and 0 <= int(sample_idx) < len(gt_df):
                                gt_row = gt_df.iloc[int(sample_idx)]
                                for gt_col in gt_columns:
                                    if gt_col in gt_row.index:
                                        det_df.at[i, f'gt_{gt_col}'] = gt_row[gt_col]
                        # æŒ‰éœ€ä¿ç•™GTåŸå§‹å­—æ®µï¼ˆä¸å†æ·»åŠ é¡¶å±‚é‡å¤å­—æ®µä»¥é¿å…å†—ä½™ï¼‰
                        # å¯¹äº fixed ç‰ˆæœ¬ï¼Œå°†ä¸æ¼æ´ç±»å‹/ä½ç½®ç›¸å…³çš„ GT å­—æ®µæ¸…ç©ºï¼Œé¿å…ä¸â€œå·²ä¿®å¤â€è¯­ä¹‰å†²çª
                        if 'sample_id' in det_df.columns:
                            vuln_location_cols = [
                                'gt_vulnerable_code_paths',
                                'gt_vulnerable_line_ranges',
                                'gt_vulnerable_function_names',
                                'gt_vulnerability_classification_breakdown',
                            ]
                            fixed_mask = det_df['sample_id'].astype(str).str.contains('_fixed')
                            for vc in vuln_location_cols:
                                if vc in det_df.columns:
                                    det_df.loc[fixed_mask, vc] = ''
                except Exception as e:
                    print(f"âš ï¸ æ·»åŠ GTå­—æ®µå¤±è´¥: {e}")
                
                # æ£€æµ‹å¯¼å‡ºç§»é™¤ä»…å†…éƒ¨ç”¨çš„æ—¶é—´æˆ³ä¸èšåˆå­—æ®µï¼›ä¿ç•™é€æ ·æœ¬ç›¸ä¼¼åº¦åŸå§‹æ•°å€¼ä¸åˆ¤å®šæ ‡å¿—
                drop_cols = [
                    'timestamp',
                    # å»é™¤æ··æ·†çŸ©é˜µç›¸å…³é€æ ·æœ¬åˆ—ï¼ˆæ£€æµ‹CSVä»…ä¿ç•™åŸå§‹GTä¸é¢„æµ‹+ç›¸ä¼¼åº¦ï¼‰
                    'project_confusion','project_tp','project_tn','project_fp','project_fn',
                    'project_precision','project_recall','project_f1',
                    'function_confusion','function_tp','function_tn','function_fp','function_fn',
                    'function_precision','function_recall','function_f1'
                ]
                for c in drop_cols:
                    if c in det_df.columns:
                        det_df = det_df.drop(columns=[c])
                if sample:
                    if 'sample_index' in det_df.columns:
                        wanted = [str(i) for i in (selected_sample_indices or set())]
                        det_sample = det_df[det_df['sample_index'].astype(str).isin(wanted)]
                        det_sample.to_csv(det_out, index=False)
                    else:
                        det_df.head(take_n * 2).to_csv(det_out, index=False)
                else:
                    det_df.to_csv(det_out, index=False)
                print(f"ğŸ“ æ£€æµ‹ç»“æœå·²å¯¼å‡º: {det_out}")
            except Exception:
                pass

            # 2) evaluation å¯¼å‡ºï¼šå†™å‡ºèšåˆåçš„å®è§‚ä¸€è¡ŒCSVï¼ˆä¸å«é€é¡¹ç›®æ˜ç»†ï¼‰
            # é€‰æ‹©å‚ä¸æ±‡æ€»çš„è¡Œï¼šsample æ¨¡å¼ä¸‹æ±‡æ€»æ‰€é€‰çš„ sample_index å…¨éƒ¨è¡Œï¼ˆé€šå¸¸4è¡Œï¼‰ï¼Œå¦åˆ™å…¨éƒ¨
            if sample:
                if 'sample_index' in res_updated.columns and selected_sample_indices:
                    wanted = [str(i) for i in selected_sample_indices]
                    agg_rows = res_updated[res_updated['sample_index'].astype(str).isin(wanted)].to_dict(orient='records')
                else:
                    agg_rows = res_updated.head(take_n * 2).to_dict(orient='records')
            else:
                agg_rows = res_updated.to_dict(orient='records')
            proj = summarize_confusions(agg_rows, 'project')
            func = summarize_confusions(agg_rows, 'function')
            sims_means = similarity_means(agg_rows)
            summary_cols = [
                'model_name','dataset_type','num_samples',
                'project_tp','project_tn','project_fp','project_fn','project_precision','project_recall','project_f1',
                'function_tp','function_tn','function_fp','function_fn','function_precision','function_recall','function_f1',
                'cosine_similarity_mean','bert_similarity_mean','rouge_score_mean'
            ]
            summary_row = {
                'model_name': model_name,
                'dataset_type': dataset_type,
                'num_samples': len(agg_rows),
                **proj,
                **func,
                'cosine_similarity_mean': sims_means.get('cosine_similarity_mean',0.0),
                'bert_similarity_mean': sims_means.get('bert_similarity_mean',0.0),
                'rouge_score_mean': sims_means.get('rouge_score_mean',0.0),
            }
            eval_out = os.path.join(export_dir, f"{safe_model}_{safe_dt}_evaluation{sample_suffix}.csv")
            pd.DataFrame([summary_row])[summary_cols].to_csv(eval_out, index=False)
            print(f"ğŸ“ è¯„ä¼°ç»“æœå·²å¯¼å‡º: {eval_out}")
        except Exception as e:
            print(f"âš ï¸ å‘½åå¯¼å‡ºå¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ é‡ç®—å¤±è´¥: {e}")


def consensus_summary(dataset_type: str = "original", target_model: str = None, theta: float = None):
    """è·¨æ¨¡å‹åŸºäºå…±è¯†(åŒ…å«ç›®æ ‡æ¨¡å‹)ä¿®æ­£çœŸå®æ ‡ç­¾ y_consï¼Œ
    å¹¶æŒ‰â€œä¸ç°æœ‰F1ç›¸åŒçš„åˆ¤å®šå£å¾„â€åˆ†åˆ«è®¡ç®—é¡¹ç›®çº§ä¸å‡½æ•°çº§çš„ Cons-Precision/Recall/F1ã€‚

    çº¦å®šï¼š
    - ä»…ç»Ÿè®¡ status == 'success' çš„è®°å½•ï¼›é”™è¯¯/ç¼ºå¤±è§†ä¸ºå¼ƒæƒï¼Œä»è¯¥æ ·æœ¬çš„ M ä¸­ç§»é™¤ã€‚
    - æ¯æ¡æ£€æµ‹è®°å½•(key = sample_index + sample_id)ç‹¬ç«‹è®¡ç®—å…±è¯†ï¼›åŒä¸€ index çš„ vulnerable/fixed åˆ†åˆ«è®¡ã€‚
    - å…±è¯†æŠ•ç¥¨ K(x) ä½¿ç”¨ has_vulnerability_pred ä¸ºæ­£ä¾‹åˆ¤æ–­ã€‚
    - ç›®æ ‡æ¨¡å‹åŒ…å«åœ¨å…±è¯†é›†åˆ M ä¸­ï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ï¼‰ã€‚
    """
    models_dir = os.path.join('evaluation', 'checkpoints', 'models')
    if not os.path.isdir(models_dir):
        print("â„¹ï¸ æœªå‘ç°æ¨¡å‹ç»“æœç›®å½•")
        return
    theta = float(theta if theta is not None else CONSENSUS_CONFIG.get('consensus_threshold', 0.6))

    # è¯»å–æ‰€æœ‰æ¨¡å‹çš„è¯¥æ•°æ®é›†ç»“æœ
    def _safe_key(row: pd.Series) -> str:
        try:
            return f"{int(row.get('sample_index'))}_{str(row.get('sample_id','')).strip()}"
        except Exception:
            return f"{str(row.get('sample_index','')).strip()}_{str(row.get('sample_id','')).strip()}"

    model_to_df: Dict[str, pd.DataFrame] = {}
    for folder in sorted(os.listdir(models_dir)):
        res_csv = os.path.join(models_dir, folder, 'results', f'evaluation_results_{dataset_type}.csv')
        if not os.path.exists(res_csv):
            continue
        try:
            df = pd.read_csv(res_csv)
            if 'status' in df.columns:
                df = df[df['status'].astype(str).str.lower() == 'success']
            if df.empty:
                continue
            df = df.copy()
            df['__key__'] = df.apply(_safe_key, axis=1)
            model_to_df[folder] = df
        except Exception:
            continue

    if not model_to_df:
        print("â„¹ï¸ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹çš„æˆåŠŸæ ·æœ¬ç»“æœ")
        return

    # å»ºç«‹å…±è¯†æ ‡ç­¾ y_cons[key]
    all_keys: set = set()
    for df in model_to_df.values():
        all_keys.update(df['__key__'].unique().tolist())

    # ä¸ºæ¯ä¸ª key è®¡ç®— M å’Œ K
    y_cons: Dict[str, int] = {}
    for key in all_keys:
        M = 0
        K = 0
        for df in model_to_df.values():
            rows = df[df['__key__'] == key]
            if rows.empty:
                continue  # è¯¥æ¨¡å‹ç¼ºå¤±æ­¤æ ·æœ¬ -> ä¸è®¡å…¥M
            M += 1
            try:
                hv = bool(rows.iloc[0].get('has_vulnerability_pred', False))
            except Exception:
                hv = False
            if hv:
                K += 1
        if M > 0:
            y_cons[key] = 1 if (K / M) >= theta else 0
        else:
            # ç†è®ºä¸Šä¸ä¼šè¿›å…¥ï¼ˆall_keysç”±å„æ¨¡å‹å¹¶é›†è€Œæ¥ï¼‰ï¼Œå®¹é”™ä¸ºè´Ÿä¾‹
            y_cons[key] = 0

    # å¯¹æ¯ä¸ªæ¨¡å‹ï¼ŒæŒ‰ä¸ç°æœ‰F1ç›¸åŒçš„y_predå£å¾„ï¼Œæ›¿æ¢çœŸå®æ ‡ç­¾ä¸º y_consï¼Œè®¡ç®—ä¸¤ä¸ªå±‚çº§çš„æ··æ·†ä¸PRF1
    def _row_to_sample_pred(row: pd.Series, y_true_cons: int) -> (Dict[str, Any], Dict[str, Any]):
        # sample_data: ä½¿ç”¨GTå­—æ®µï¼Œä½† has_vulnerability ç”±å…±è¯†æ›¿æ¢
        sample_data = {
            'has_vulnerability': bool(y_true_cons),
            'vulnerability_classification': str(row.get('vulnerability_type_gt', '') or ''),
            'vulnerability_type': str(row.get('vulnerability_type_gt', '') or ''),
            'vulnerable_code_paths': str(row.get('vulnerable_code_paths', '') or ''),
            'vulnerable_function_names': str(row.get('vulnerable_function_names', '') or ''),
            'file_path': str(row.get('file_path', '') or ''),
            'function_name': str(row.get('function_name', '') or ''),
            'files': str(row.get('files', '') or ''),
        }
        llm_result = {
            'has_vulnerability': bool(row.get('has_vulnerability_pred', False)),
            'vulnerability_type': str(row.get('vulnerability_type_pred', '') or ''),
            'filename': str(row.get('filename_pred', '') or ''),
            'function_name': str(row.get('function_name_pred', '') or ''),
        }
        return sample_data, llm_result

    # å…è®¸æŒ‡å®š target_modelï¼Œä»…è¾“å‡ºè¯¥æ¨¡å‹ï¼›å¦åˆ™è¾“å‡ºæ‰€æœ‰
    selected_models = [target_model] if target_model else list(model_to_df.keys())
    # æ­£å¸¸åŒ–é€‰æ‹©ï¼ˆç”¨æ–‡ä»¶å¤¹ååŒ¹é…ï¼‰
    selected_models = [m for m in selected_models if m in model_to_df] if target_model else selected_models
    if target_model and not selected_models:
        print(f"âŒ ç›®æ ‡æ¨¡å‹æœªæ‰¾åˆ°: {target_model}")
        return

    print("\n===== Consensus-F1 Summary =====")
    print(f"Dataset: {dataset_type}  Theta: {theta}")
    for folder in selected_models:
        df = model_to_df[folder]
        # å‹å¥½åç§°ï¼šä¼˜å…ˆCSVä¸­çš„model_name
        model_friendly = None
        try:
            if 'model_name' in df.columns and not df.empty:
                model_friendly = str(df.iloc[0].get('model_name', folder))
        except Exception:
            model_friendly = None
        model_label = model_friendly or folder

        proj_tp = proj_tn = proj_fp = proj_fn = 0
        func_tp = func_tn = func_fp = func_fn = 0

        for _, row in df.iterrows():
            key = row['__key__']
            y_true = int(y_cons.get(key, 0))
            sample_data, llm_result = _row_to_sample_pred(row, y_true)
            # é¡¹ç›®çº§
            _, proj_counts = compute_project_tuple_confusion(sample_data, llm_result)
            proj_tp += int(proj_counts.get('project_tp', 0))
            proj_tn += int(proj_counts.get('project_tn', 0))
            proj_fp += int(proj_counts.get('project_fp', 0))
            proj_fn += int(proj_counts.get('project_fn', 0))
            # å‡½æ•°çº§
            _, func_counts = compute_function_quad_confusion(sample_data, llm_result)
            func_tp += int(func_counts.get('function_tp', 0))
            func_tn += int(func_counts.get('function_tn', 0))
            func_fp += int(func_counts.get('function_fp', 0))
            func_fn += int(func_counts.get('function_fn', 0))

        p_p, r_p, f1_p = precision_recall_f1(proj_tp, proj_fp, proj_fn)
        p_f, r_f, f1_f = precision_recall_f1(func_tp, func_fp, func_fn)

        print(f"\nModel: {model_label}")
        print("Project-level (consensus true labels):")
        print(f"  TP: {proj_tp}  TN: {proj_tn}  FP: {proj_fp}  FN: {proj_fn}")
        print(f"  Precision: {p_p:.3f}  Recall: {r_p:.3f}  F1: {f1_p:.3f}")
        print("Function-level (consensus true labels):")
        print(f"  TP: {func_tp}  TN: {func_tn}  FP: {func_fp}  FN: {func_fn}")
        print(f"  Precision: {p_f:.3f}  Recall: {r_f:.3f}  F1: {f1_f:.3f}")


def export_results(output_path: str = None):
    """å¯¼å‡ºè¯„ä¼°ç»“æœ"""
    checkpoint_manager = CheckpointManager()
    
    if output_path:
        output_file = checkpoint_manager.export_results(output_path)
    else:
        output_file = checkpoint_manager.export_results()
    
    if output_file:
        print(f"âœ… ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
    else:
        print("âŒ å¯¼å‡ºå¤±è´¥")


def cleanup_checkpoints(max_age_days: int = 7):
    """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoint_manager = CheckpointManager()
    checkpoint_manager.cleanup_old_checkpoints(max_age_days)
    print(f"ğŸ§¹ å·²æ¸…ç† {max_age_days} å¤©å‰çš„æ£€æŸ¥ç‚¹æ–‡ä»¶")


def reset_evaluation():
    """é‡ç½®è¯„ä¼°è¿›åº¦"""
    checkpoint_manager = CheckpointManager()
    
    # å¤‡ä»½å½“å‰è¿›åº¦
    progress_file = checkpoint_manager.progress_file
    if os.path.exists(progress_file):
        backup_file = f"{progress_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.rename(progress_file, backup_file)
        print(f"ğŸ“‹ å½“å‰è¿›åº¦å·²å¤‡ä»½åˆ°: {backup_file}")
    
    # é‡ç½®è¿›åº¦
    checkpoint_manager.progress = {
        "evaluation_id": "",
        "start_time": "",
        "dataset_type": "",
        "model_name": "",
        "total_samples": 0,
        "completed_samples": [],
        "failed_samples": [],
        "current_sample_index": 0,
        "last_updated": ""
    }
    checkpoint_manager._save_progress()
    print("ğŸ”„ è¯„ä¼°è¿›åº¦å·²é‡ç½®")


def show_checkpoint_files():
    """æ˜¾ç¤ºæ£€æŸ¥ç‚¹æ–‡ä»¶ä¿¡æ¯"""
    checkpoint_manager = CheckpointManager()
    
    print("ğŸ“ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¿¡æ¯:")
    print(f"  æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_manager.checkpoint_dir}")
    print(f"  è¿›åº¦æ–‡ä»¶: {checkpoint_manager.progress_file}")
    print(f"  ç»“æœæ–‡ä»¶: {checkpoint_manager.results_csv}")
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if os.path.exists(checkpoint_manager.progress_file):
        progress_size = os.path.getsize(checkpoint_manager.progress_file)
        progress_time = datetime.fromtimestamp(os.path.getmtime(checkpoint_manager.progress_file))
        print(f"  è¿›åº¦æ–‡ä»¶: å­˜åœ¨ ({progress_size} bytes, ä¿®æ”¹æ—¶é—´: {progress_time})")
    else:
        print("  è¿›åº¦æ–‡ä»¶: ä¸å­˜åœ¨")
    
    if os.path.exists(checkpoint_manager.results_csv):
        results_size = os.path.getsize(checkpoint_manager.results_csv)
        results_time = datetime.fromtimestamp(os.path.getmtime(checkpoint_manager.results_csv))
        print(f"  ç»“æœæ–‡ä»¶: å­˜åœ¨ ({results_size} bytes, ä¿®æ”¹æ—¶é—´: {results_time})")
    else:
        print("  ç»“æœæ–‡ä»¶: ä¸å­˜åœ¨")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ–­ç‚¹ç»­ä¼ ç®¡ç†å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')
    
    # æ˜¾ç¤ºè¿›åº¦å‘½ä»¤
    progress_parser = subparsers.add_parser('progress', help='æ˜¾ç¤ºå½“å‰è¯„ä¼°è¿›åº¦')
    progress_parser.add_argument('--model', type=str, default="deepseek-ai/DeepSeek-V3.1", help='æ¨¡å‹åç§°')
    progress_parser.add_argument('--dataset-type', type=str, default="original", help='æ•°æ®é›†ç±»å‹')
    progress_parser.add_argument('--include-errors', action='store_true', help='å°† error è®¡å…¥å‰©ä½™ï¼ˆremainingï¼‰')
    # åŒæ­¥è¿›åº¦å‘½ä»¤
    sync_parser = subparsers.add_parser('sync', help='å°†results CSVä¸­çš„success/erroråŒæ­¥åˆ°è¿›åº¦æ–‡ä»¶')
    sync_parser.add_argument('--model', type=str, default="deepseek-ai/DeepSeek-V3.1", help='æ¨¡å‹åç§°')
    sync_parser.add_argument('--dataset-type', type=str, default="original", help='æ•°æ®é›†ç±»å‹')
    
    # æ˜¾ç¤ºç»“æœå‘½ä»¤
    results_parser = subparsers.add_parser('results', help='æ˜¾ç¤ºè¯„ä¼°ç»“æœ')
    # æ±‡æ€»
    summary_parser = subparsers.add_parser('summary', help='æ±‡æ€»å½“å‰ç»“æœçš„TP/TN/FP/FNä¸P/R/F1åŠè¯­ä¹‰ç›¸ä¼¼ç‡')
    summary_parser.add_argument('--model', type=str, default="deepseek-ai/DeepSeek-V3.1", help='æ¨¡å‹åç§°')
    summary_parser.add_argument('--dataset-type', type=str, default="original", help='æ•°æ®é›†ç±»å‹')
    # é‡ç®—æ–°å¢æŒ‡æ ‡
    recompute_parser = subparsers.add_parser('recompute', help='é‡ç®—å¹¶å¡«å……ç»“æœCSVä¸­çš„æ–°å¢è¯„ä¼°æŒ‡æ ‡åˆ—')
    recompute_parser.add_argument('--model', type=str, default="deepseek-ai/DeepSeek-V3.1", help='æ¨¡å‹åç§°')
    recompute_parser.add_argument('--dataset-type', type=str, default=None, help='æ•°æ®é›†ç±»å‹ï¼ˆé»˜è®¤ä»è¿›åº¦ä¸­è¯»å–ï¼‰')
    recompute_parser.add_argument('--sample', action='store_true', help='ä»…å¯¹å°‘é‡æ ·æœ¬æ‰§è¡Œï¼ˆevaluation_idè¿½åŠ  _sample æ ‡è®°ï¼‰')
    recompute_parser.add_argument('--sample-from-dataset', action='store_true', help='ä»æ•°æ®é›†å‰Næ¡ä¸­é€‰æ‹©æ ·æœ¬ï¼ˆåŒ¹é… sample_indexï¼‰ï¼Œå¦åˆ™ä»æ£€æµ‹CSVå‰Næ¡é€‰æ‹©')
    recompute_parser.add_argument('--sample-size', type=int, default=2, help='é‡‡æ ·æ¡æ•°ï¼ˆé»˜è®¤2ï¼‰')
    # å¤šæ¨¡å‹å…±è¯†
    consensus_parser = subparsers.add_parser('consensus', help='åŸºäºå¤šæ¨¡å‹å…±è¯†è®¡ç®—Cons-F1æ‘˜è¦ï¼ˆé¡¹ç›®çº§/å‡½æ•°çº§ï¼‰')
    consensus_parser.add_argument('--dataset-type', type=str, default='original', help='æ•°æ®é›†ç±»å‹ï¼ˆä¸å¸¸è§„F1ä¸€è‡´ï¼‰')
    consensus_parser.add_argument('--target-model', type=str, default=None, help='ä»…è¾“å‡ºæŒ‡å®šæ¨¡å‹ï¼ˆæ–‡ä»¶å¤¹åï¼Œå¦‚ deepseek-ai_DeepSeek-V3.1ï¼‰')
    consensus_parser.add_argument('--theta', type=float, default=None, help='å…±è¯†é˜ˆå€¼ï¼Œé»˜è®¤0.6ï¼ˆå¯æŒ‰æ•°æ®é›†è‡ªè¡ŒæŒ‡å®šï¼‰')
    
    # å¯¼å‡ºç»“æœå‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡ºè¯„ä¼°ç»“æœ')
    export_parser.add_argument('--output', '-o', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # æ¸…ç†æ£€æŸ¥ç‚¹å‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶')
    cleanup_parser.add_argument('--days', '-d', type=int, default=7, help='ä¿ç•™å¤©æ•°')
    
    # é‡ç½®è¯„ä¼°å‘½ä»¤
    reset_parser = subparsers.add_parser('reset', help='é‡ç½®è¯„ä¼°è¿›åº¦')
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯å‘½ä»¤
    files_parser = subparsers.add_parser('files', help='æ˜¾ç¤ºæ£€æŸ¥ç‚¹æ–‡ä»¶ä¿¡æ¯')
    
    args = parser.parse_args()
    
    if args.command == 'progress':
        show_progress(
            getattr(args, 'model', "deepseek-ai/DeepSeek-V3.1"),
            getattr(args, 'dataset_type', "original"),
            getattr(args, 'include_errors', False)
        )
    elif args.command == 'sync':
        sync_progress_from_results(
            getattr(args, 'model', "deepseek-ai/DeepSeek-V3.1"),
            getattr(args, 'dataset_type', "original")
        )
    elif args.command == 'results':
        show_results()
    elif args.command == 'summary':
        show_summary(
            getattr(args, 'model', "deepseek-ai/DeepSeek-V3.1"),
            getattr(args, 'dataset_type', "original")
        )
    elif args.command == 'recompute':
        recompute_metrics(
            getattr(args, 'dataset_type', None),
            getattr(args, 'sample', False),
            getattr(args, 'sample_from_dataset', False),
            getattr(args, 'sample_size', 2),
            getattr(args, 'model', "deepseek-ai/DeepSeek-V3.1"),
        )
    elif args.command == 'consensus':
        consensus_summary(
            getattr(args, 'dataset_type', 'original'),
            getattr(args, 'target_model', None),
            getattr(args, 'theta', None),
        )
    elif args.command == 'export':
        export_results(args.output)
    elif args.command == 'cleanup':
        cleanup_checkpoints(args.days)
    elif args.command == 'reset':
        reset_evaluation()
    elif args.command == 'files':
        show_checkpoint_files()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
